{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scientific Data Analysis\n",
    "Jasper Wink, 14616513 \\\n",
    "Dennis van der Werff, 14562189"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import ast\n",
    "\n",
    "from statsmodels.tsa.stattools import adfuller, grangercausalitytests\n",
    "from statsmodels.tsa.api import VAR\n",
    "import statsmodels.api as sm\n",
    "import warnings\n",
    "\n",
    "# There are irrelevant warning that make the output unreadable/less readable.\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "cn = pd.read_csv(\"data/cryptonews.csv\")\n",
    "bp = pd.read_csv(\"data/btc.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the date to the datetime format.\n",
    "cn['date'] = pd.to_datetime(cn['date'], format='%Y-%m-%d %H:%M:%S')\n",
    "bp['date'] = pd.to_datetime(bp['Date'], format='%Y-%m-%d')\n",
    "\n",
    "# Discard unnecairy date data, eg. hours, minutes etc.\n",
    "cn['weeks'] = cn['date'].dt.to_period('W')\n",
    "bp['weeks'] = bp['date'].dt.to_period('W')\n",
    "\n",
    "# Extract the polarity field out of the cn dataset.\n",
    "cn['sentiment'] = cn['sentiment'].apply(ast.literal_eval)\n",
    "cn['polarity'] = cn['sentiment'].apply(lambda x: x['polarity'])\n",
    "\n",
    "# Group bitcoin data by the weeks and merge into new dataframe.\n",
    "bp_weekly_open = bp.groupby('weeks')['Open'].mean().reset_index(name='open_mean')\n",
    "bp_weekly_volume = bp.groupby('weeks')['Volume'].mean().reset_index(name='volume_mean')\n",
    "bp_weekly_data = pd.merge(bp_weekly_open, bp_weekly_volume, on='weeks', how='inner')\n",
    "\n",
    "# Group crpyto news data by the weeks and merge into new dataframe.\n",
    "cn_weekly_count = cn.groupby('weeks').size().reset_index(name='count')\n",
    "cn_weekly_sentiment = cn.groupby('weeks')['polarity'].mean().reset_index(name='polarity_mean')\n",
    "cn_weekly_data = pd.merge(cn_weekly_count, cn_weekly_sentiment, on='weeks', how='inner')\n",
    "\n",
    "# Merge all weekly data into 1 big dataframe.\n",
    "weekly_data = pd.merge(bp_weekly_data, cn_weekly_data, on='weeks', how='inner')\n",
    "print(weekly_data.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the data\n",
    "count_list = list(weekly_data['count'])\n",
    "normalized_count = [count / max(count_list) for count in count_list]\n",
    "\n",
    "sentiment_list = list(weekly_data['polarity_mean'])\n",
    "normalized_sentiment = [sentiment / max(sentiment_list) for sentiment in sentiment_list]\n",
    "\n",
    "open_list = list(weekly_data['open_mean'])\n",
    "normalized_price = [open / max(open_list) for open in open_list]\n",
    "\n",
    "volume_list = list(weekly_data['volume_mean'])\n",
    "normalized_volume = [volume / max(volume_list) for volume in volume_list]\n",
    "\n",
    "\n",
    "x = np.linspace(0, len(normalized_count), len(normalized_count))\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Bitcoin Price vs Article Count\n",
    "axes[0, 0].plot(x, normalized_price, label='Bitcoin price', color='red')\n",
    "axes[0, 0].plot(x, normalized_count, label='Article count', color='orange')\n",
    "axes[0, 0].set_title('Bitcoin price vs Article count')\n",
    "axes[0, 0].set_xlabel('Weeks')\n",
    "axes[0, 0].set_ylabel('Normalized values')\n",
    "axes[0, 0].legend()\n",
    "\n",
    "# Bitcoin Volume vs Article Count\n",
    "axes[0, 1].plot(x, normalized_volume, label='Bitcoin volume', color='blue')\n",
    "axes[0, 1].plot(x, normalized_count, label='Article count', color='orange')\n",
    "axes[0, 1].set_title('Bitcoin volume vs Article count')\n",
    "axes[0, 1].set_xlabel('Weeks')\n",
    "axes[0, 1].set_ylabel('Normalized values')\n",
    "axes[0, 1].legend()\n",
    "\n",
    "# Bitcoin Price vs Sentiment\n",
    "axes[1, 0].plot(x, normalized_price, label='Bitcoin price', color='red')\n",
    "axes[1, 0].plot(x, normalized_sentiment, label='Sentiment', color='green')\n",
    "axes[1, 0].set_title('Bitcoin Price vs Sentiment')\n",
    "axes[1, 0].set_xlabel('Weeks')\n",
    "axes[1, 0].set_ylabel('Normalized values')\n",
    "axes[1, 0].legend()\n",
    "\n",
    "# Bitcoin Volume vs Sentiment\n",
    "axes[1, 1].plot(x, normalized_volume, label='Bitcoin volume', color='blue')\n",
    "axes[1, 1].plot(x, normalized_sentiment, label='Sentiment', color='green')\n",
    "axes[1, 1].set_title('Bitcoin volume vs Sentiment')\n",
    "axes[1, 1].set_xlabel('Weeks')\n",
    "axes[1, 1].set_ylabel('Normalized values')\n",
    "axes[1, 1].legend()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stationary test\n",
    "\n",
    "Using the Augmented Dickey-Fuller (ADF) test to check if the current data is satationary. \\\n",
    "If the p-value scores lower than 0.05 we reject the $H_0$ and determine that the data is stationary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test if the adfuller test scores lower than p-value = 0.05.\n",
    "def test_stationary(data, name):\n",
    "    ad_fuller_result = adfuller(data.dropna())\n",
    "    if ad_fuller_result[1] > 0.05:\n",
    "        print(f'{name} is not stationary | p-value: {ad_fuller_result[1]}')\n",
    "    else:\n",
    "        print(f'{name} is stationary | p-value: {ad_fuller_result[1]}')\n",
    "\n",
    "\n",
    "# Prepair the data, by differentiating if needed. (Making it stationary)\n",
    "weekly_data['open_diff'] = weekly_data['open_mean'].diff()\n",
    "test_stationary(weekly_data['open_diff'], 'Bitcoin price')\n",
    "\n",
    "weekly_data['volume_diff'] = weekly_data['volume_mean'].diff()\n",
    "test_stationary(weekly_data['volume_diff'], 'Bitcoin volume')\n",
    "\n",
    "test_stationary(weekly_data['count'], 'Article count')\n",
    "\n",
    "test_stationary(weekly_data['polarity_mean'], 'News polarity')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stationary Data visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the data\n",
    "count_list = list(weekly_data['count'][1:])\n",
    "normalized_count = [count / max(count_list) for count in count_list]\n",
    "\n",
    "sentiment_list = list(weekly_data['polarity_mean'][1:])\n",
    "normalized_sentiment = [sentiment / max(sentiment_list) for sentiment in sentiment_list]\n",
    "\n",
    "open_list = list(weekly_data['open_diff'].dropna())\n",
    "normalized_price = [open / max(open_list) for open in open_list]\n",
    "\n",
    "volume_list = list(weekly_data['volume_diff'].dropna())\n",
    "normalized_volume = [volume / max(volume_list) for volume in volume_list]\n",
    "\n",
    "\n",
    "x = np.linspace(0, len(normalized_count), len(normalized_count))\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Bitcoin Price vs Article Count\n",
    "axes[0, 0].plot(x, normalized_price, label='Bitcoin price', color='red')\n",
    "axes[0, 0].plot(x, normalized_count, label='Article count', color='orange')\n",
    "axes[0, 0].set_title('Bitcoin price vs Article count')\n",
    "axes[0, 0].set_xlabel('Weeks')\n",
    "axes[0, 0].set_ylabel('Normalized values')\n",
    "axes[0, 0].legend()\n",
    "\n",
    "# Bitcoin Volume vs Article Count\n",
    "axes[0, 1].plot(x, normalized_volume, label='Bitcoin volume', color='blue')\n",
    "axes[0, 1].plot(x, normalized_count, label='Article count', color='orange')\n",
    "axes[0, 1].set_title('Bitcoin volume vs Article count')\n",
    "axes[0, 1].set_xlabel('Weeks')\n",
    "axes[0, 1].set_ylabel('Normalized values')\n",
    "axes[0, 1].legend()\n",
    "\n",
    "# Bitcoin Price vs Sentiment\n",
    "axes[1, 0].plot(x, normalized_price, label='Bitcoin price', color='red')\n",
    "axes[1, 0].plot(x, normalized_sentiment, label='Sentiment', color='green')\n",
    "axes[1, 0].set_title('Bitcoin Price vs Sentiment')\n",
    "axes[1, 0].set_xlabel('Weeks')\n",
    "axes[1, 0].set_ylabel('Normalized values')\n",
    "axes[1, 0].legend()\n",
    "\n",
    "# Bitcoin Volume vs Sentiment\n",
    "axes[1, 1].plot(x, normalized_volume, label='Bitcoin volume', color='blue')\n",
    "axes[1, 1].plot(x, normalized_sentiment, label='Sentiment', color='green')\n",
    "axes[1, 1].set_title('Bitcoin volume vs Sentiment')\n",
    "axes[1, 1].set_xlabel('Weeks')\n",
    "axes[1, 1].set_ylabel('Normalized values')\n",
    "axes[1, 1].legend()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spearman Correlation\n",
    "To be able to detect whether there is correlation between the bitcoin and the news we will be using spearman correlation. \\\n",
    "The correlation can score between -1 and 1 where 1 means a perfect correlation and -1 means a perfect inverse correlation. \\\n",
    "A score of 0 means there is no correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://en.wikipedia.org/wiki/Spearman's_rank_correlation_coefficient\n",
    "def spearman_correlation(rank1, rank2):\n",
    "    n = len(rank1)\n",
    "\n",
    "    sum_d_squared = sum([(rank1[i] - rank2[i])**2 for i in range(n)])\n",
    "    correlation = 1 - ((6 * sum_d_squared) / (n * (n**2 - 1)))\n",
    "    return correlation\n",
    "\n",
    "\n",
    "# Calculate ranks\n",
    "bp_open_rank = list(weekly_data['open_diff'].dropna().rank())\n",
    "bp_volume_rank = list(weekly_data['volume_diff'].dropna().rank())\n",
    "cn_count_rank = list(weekly_data['count'][1:].rank())\n",
    "cn_sentiment_rank = list(weekly_data['polarity_mean'][1:].rank())\n",
    "\n",
    "pairs = [[bp_open_rank, cn_count_rank], [bp_volume_rank, cn_count_rank],\n",
    "         [bp_open_rank, cn_sentiment_rank], [bp_volume_rank, cn_sentiment_rank]]\n",
    "\n",
    "names = ['open price vs article count', 'volume vs article count',\n",
    "         'open price vs sentiment', 'volume vs sentiment']\n",
    "\n",
    "# Calculate the spearman correlation and print it.\n",
    "for i, pair in enumerate(pairs):\n",
    "    correlation = spearman_correlation(pair[0], pair[1])\n",
    "    print(f'{names[i]}')\n",
    "    print(f'Spearman Correlation: {correlation:.3f}\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector AutoRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getVar(data1, data2, n, weekly_data):\n",
    "    data = weekly_data[[data1, data2]].dropna()\n",
    "    model = VAR(data)\n",
    "    lags = {data1: [], data2: []}\n",
    "\n",
    "    # Check what lags are significant for 0 to n max lags.\n",
    "    for i in range(n):\n",
    "        coef1 = []\n",
    "        coef2 = []\n",
    "        model_fit = model.fit(maxlags=i)\n",
    "\n",
    "        coefs = model_fit.coefs\n",
    "        pvalues = model_fit.pvalues_endog_lagged\n",
    "\n",
    "        # Check if a lag is significant (p-value < 0.05).\n",
    "        for j in range(i):\n",
    "            if pvalues[j*2][0] < 0.05:\n",
    "                coef1.append((j+1, coefs[j][0][0]))\n",
    "            if pvalues[j*2+1][0] < 0.05:\n",
    "                coef2.append((j+1, coefs[j][0][1]))\n",
    "\n",
    "        # Store the significant lags and coeficients.\n",
    "        lags[data1].append(coef1)\n",
    "        lags[data2].append(coef2)\n",
    "\n",
    "    return lags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example to show output\n",
    "data = weekly_data[['count', 'open_diff']].dropna()\n",
    "model = VAR(data)\n",
    "model_fit = model.fit(maxlags=5)\n",
    "model_fit.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{The resulting equation for the example is as follows:}$ \\\n",
    "$C_t = 0.675 \\cdot C_{t-1} + 0.004 \\cdot O_{t-1} - 0.005 \\cdot O_{t-2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Akaike Information Criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getBestLag(data1, data2, n, weekly_data):\n",
    "    min = (0, np.inf)\n",
    "    lags = getVar(data1, data2, n, weekly_data)\n",
    "\n",
    "    for i in range(n):\n",
    "        x_columns_str = ''\n",
    "\n",
    "        # Create all lag columns and multiply them by their coeficient.\n",
    "        for key in lags.keys():\n",
    "\n",
    "            for lag, coeff in lags[key][i]:\n",
    "                weekly_data[f'{key}_t-{lag}'] = coeff * weekly_data[f'{key}'].shift(lag)\n",
    "                x_columns_str += f'{key}_t-{lag} '\n",
    "\n",
    "        # Create a list of selected column names so they can be selected.\n",
    "        x_columns_list = x_columns_str.split(' ')[:-1]\n",
    "\n",
    "        x = weekly_data[x_columns_list].dropna()\n",
    "        y = weekly_data[data2][-len(x):]\n",
    "\n",
    "        # Calculate the AIC using the Ordinairy Least Squares model.\n",
    "        x = sm.add_constant(x)\n",
    "        model = sm.OLS(y, x).fit()\n",
    "        # Store the best (lowest scoring) model.\n",
    "        min = min if min[1] <= model.aic else (i, model.aic)\n",
    "\n",
    "    return min[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Granger Causality Tests\n",
    "To determine if one variable causes a change in another variable we are using Granger causality. \\\n",
    "If the p-value of a lag is lower than 0.05 we can determine that this lag influences the variable Thus Granger causes the other varible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the grangercausalitytests to check if data 1 Granger Causes data2.\n",
    "def granger_causality_test(data1, data2, max_VAR_lags):\n",
    "    data = weekly_data[[data1, data2]].dropna()\n",
    "    best_lag = getBestLag(data1, data2, max_VAR_lags, weekly_data)\n",
    "    granger_result = grangercausalitytests(data, maxlag=best_lag, verbose=False)\n",
    "    return granger_result\n",
    "\n",
    "\n",
    "# Print if data 1 Granger Causes data 2.\n",
    "def check_granger_cause(result, data1, data2):\n",
    "    for _, result in result.items():\n",
    "        p_value = result[0]['ssr_ftest'][1]\n",
    "        if p_value < 0.05:\n",
    "            print(f'+ {data1} Gragner Causes {data2}')\n",
    "            return\n",
    "\n",
    "    print(f'- {data1} does not Gragner Cause {data2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of grangercausalitytest show output\n",
    "data = weekly_data[['count', 'open_diff']].dropna()\n",
    "_ = grangercausalitytests(data, [5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Granger Causality Parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combinations = [\n",
    "    ['count', 'open_diff'],\n",
    "    ['count', 'volume_diff'],\n",
    "    ['polarity_mean', 'open_diff'],\n",
    "    ['polarity_mean', 'volume_diff']\n",
    "    ]\n",
    "max_VAR_lags = 20\n",
    "\n",
    "# Make a list with all combinations and their inverse combinations.\n",
    "inverse_combinations = [[value[1], value[0]] for value in combinations]\n",
    "all_combinations = combinations + inverse_combinations\n",
    "\n",
    "# Find the best lags and test if data1 Granger Causes data 2.\n",
    "for data1, data2 in all_combinations:\n",
    "    granger_result = granger_causality_test(data1, data2, max_VAR_lags)\n",
    "    check_granger_cause(granger_result, data1, data2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
