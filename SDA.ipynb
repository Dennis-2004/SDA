{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scientific Data Analysis\n",
    "Jasper Wink, 14616513 \\\n",
    "Dennis van der Werff, 1462189"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import ast\n",
    "\n",
    "from statsmodels.tsa.stattools import adfuller, grangercausalitytests\n",
    "from statsmodels.tsa.api import VAR\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.graphics.tsaplots import plot_pacf\n",
    "from statsmodels.tsa.api import VAR\n",
    "import statsmodels.api as sm \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "cn = pd.read_csv(\"data/cryptonews.csv\")\n",
    "bp = pd.read_csv(\"data/btc.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Group the data for each month. \\\n",
    "Determine the amount of articles for each month. \\\n",
    "Determine the mean opening bitcoin price for each month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the date to the datetime format\n",
    "cn['date'] = pd.to_datetime(cn['date'], format='%Y-%m-%d %H:%M:%S')\n",
    "bp['date'] = pd.to_datetime(bp['Date'], format='%Y-%m-%d')\n",
    "\n",
    "# Discard unnecairy date data, eg. hours, minutes etc\n",
    "cn['weeks'] = cn['date'].dt.to_period('W')\n",
    "bp['weeks'] = bp['date'].dt.to_period('W')\n",
    "\n",
    "# Extract the polarity field out of the cn dataset\n",
    "cn['sentiment'] = cn['sentiment'].apply(ast.literal_eval)\n",
    "cn['polarity'] = cn['sentiment'].apply(lambda x: x['polarity'])\n",
    "\n",
    "# Group data by the weeks\n",
    "bp_weekly_open = bp.groupby('weeks')['Open'].mean().reset_index(name='open_mean')\n",
    "bp_weekly_volume = bp.groupby('weeks')['Volume'].mean().reset_index(name='volume_mean')\n",
    "bp_weekly_data = pd.merge(bp_weekly_open, bp_weekly_volume, on='weeks', how='inner')\n",
    "\n",
    "cn_weekly_count = cn.groupby('weeks').size().reset_index(name='count')\n",
    "cn_weekly_sentiment = cn.groupby('weeks')['polarity'].mean().reset_index(name='polarity_mean')\n",
    "cn_weekly_data = pd.merge(cn_weekly_count, cn_weekly_sentiment, on='weeks', how='inner')\n",
    "\n",
    "weekly_data = pd.merge(bp_weekly_data, cn_weekly_data, on='weeks', how='inner')\n",
    "print(weekly_data.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the data\n",
    "count_list = list(weekly_data['count'])\n",
    "normalized_count = [count / max(count_list) for count in count_list]\n",
    "\n",
    "sentiment_list = list(weekly_data['polarity_mean'])\n",
    "normalized_sentiment = [sentiment / max(sentiment_list) for sentiment in sentiment_list]\n",
    "\n",
    "open_list = list(weekly_data['open_mean'])\n",
    "normalized_price = [open / max(open_list) for open in open_list]\n",
    "\n",
    "volume_list = list(weekly_data['volume_mean'])\n",
    "normalized_volume = [volume / max(volume_list) for volume in volume_list]\n",
    "\n",
    "\n",
    "x = np.linspace(0, len(normalized_count), len(normalized_count))\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Bitcoin Price vs Article Count\n",
    "axes[0, 0].plot(x, normalized_price, label='Bitcoin price', color='red')\n",
    "axes[0, 0].plot(x, normalized_count, label='Article count', color='orange')\n",
    "axes[0, 0].set_title('Bitcoin price vs Article count')\n",
    "axes[0, 0].set_xlabel('Weeks')\n",
    "axes[0, 0].set_ylabel('Normalized values')\n",
    "axes[0, 0].legend()\n",
    "\n",
    "# Bitcoin Volume vs Article Count\n",
    "axes[0, 1].plot(x, normalized_volume, label='Bitcoin volume', color='blue')\n",
    "axes[0, 1].plot(x, normalized_count, label='Article count', color='orange')\n",
    "axes[0, 1].set_title('Bitcoin volume vs Article count')\n",
    "axes[0, 1].set_xlabel('Weeks')\n",
    "axes[0, 1].set_ylabel('Normalized values')\n",
    "axes[0, 1].legend()\n",
    "\n",
    "# Bitcoin Price vs Sentiment\n",
    "axes[1, 0].plot(x, normalized_price, label='Bitcoin price', color='red')\n",
    "axes[1, 0].plot(x, normalized_sentiment, label='Sentiment', color='green')\n",
    "axes[1, 0].set_title('Bitcoin Price vs Sentiment')\n",
    "axes[1, 0].set_xlabel('Weeks')\n",
    "axes[1, 0].set_ylabel('Normalized values')\n",
    "axes[1, 0].legend()\n",
    "\n",
    "# Bitcoin Volume vs Sentiment\n",
    "axes[1, 1].plot(x, normalized_volume, label='Bitcoin volume', color='blue')\n",
    "axes[1, 1].plot(x, normalized_sentiment, label='Sentiment', color='green')\n",
    "axes[1, 1].set_title('Bitcoin volume vs Sentiment')\n",
    "axes[1, 1].set_xlabel('Weeks')\n",
    "axes[1, 1].set_ylabel('Normalized values')\n",
    "axes[1, 1].legend()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stationary test\n",
    "\n",
    "Using the Augmented Dickey-Fuller (ADF) test to check if the current data is satationary. If the p-value scores lower than 0.05 we reject the $H_0$ and determine that the data is stationary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test if the adfuller test scores lower than p-value = 0.05.\n",
    "def test_stationary(data, name):\n",
    "    ad_fuller_result = adfuller(data.dropna())\n",
    "    if ad_fuller_result[1] > 0.05:\n",
    "        print(f'{name} is not stationary | p-value: {ad_fuller_result[1]}')\n",
    "    else:\n",
    "        print(f'{name} is stationary | p-value: {ad_fuller_result[1]}')\n",
    "\n",
    "\n",
    "# Prepair the data, by differentiating if needed.\n",
    "weekly_data['open_diff'] = weekly_data['open_mean'].diff()\n",
    "test_stationary(weekly_data['open_diff'], 'Bitcoin price')\n",
    "\n",
    "weekly_data['volume_diff'] = weekly_data['volume_mean'].diff()\n",
    "test_stationary(weekly_data['volume_diff'], 'Bitcoin volume')\n",
    "\n",
    "test_stationary(weekly_data['count'], 'Article count')\n",
    "\n",
    "test_stationary(weekly_data['polarity_mean'], 'News polarity')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stationary Data visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the data\n",
    "count_list = list(weekly_data['count'][1:])\n",
    "normalized_count = [count / max(count_list) for count in count_list]\n",
    "\n",
    "sentiment_list = list(weekly_data['polarity_mean'][1:])\n",
    "normalized_sentiment = [sentiment / max(sentiment_list) for sentiment in sentiment_list]\n",
    "\n",
    "open_list = list(weekly_data['open_diff'].dropna())\n",
    "normalized_price = [open / max(open_list) for open in open_list]\n",
    "\n",
    "volume_list = list(weekly_data['volume_diff'].dropna())\n",
    "normalized_volume = [volume / max(volume_list) for volume in volume_list]\n",
    "\n",
    "\n",
    "x = np.linspace(0, len(normalized_count), len(normalized_count))\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Bitcoin Price vs Article Count\n",
    "axes[0, 0].plot(x, normalized_price, label='Bitcoin price', color='red')\n",
    "axes[0, 0].plot(x, normalized_count, label='Article count', color='orange')\n",
    "axes[0, 0].set_title('Bitcoin price vs Article count')\n",
    "axes[0, 0].set_xlabel('Weeks')\n",
    "axes[0, 0].set_ylabel('Normalized values')\n",
    "axes[0, 0].legend()\n",
    "\n",
    "# Bitcoin Volume vs Article Count\n",
    "axes[0, 1].plot(x, normalized_volume, label='Bitcoin volume', color='blue')\n",
    "axes[0, 1].plot(x, normalized_count, label='Article count', color='orange')\n",
    "axes[0, 1].set_title('Bitcoin volume vs Article count')\n",
    "axes[0, 1].set_xlabel('Weeks')\n",
    "axes[0, 1].set_ylabel('Normalized values')\n",
    "axes[0, 1].legend()\n",
    "\n",
    "# Bitcoin Price vs Sentiment\n",
    "axes[1, 0].plot(x, normalized_price, label='Bitcoin price', color='red')\n",
    "axes[1, 0].plot(x, normalized_sentiment, label='Sentiment', color='green')\n",
    "axes[1, 0].set_title('Bitcoin Price vs Sentiment')\n",
    "axes[1, 0].set_xlabel('Weeks')\n",
    "axes[1, 0].set_ylabel('Normalized values')\n",
    "axes[1, 0].legend()\n",
    "\n",
    "# Bitcoin Volume vs Sentiment\n",
    "axes[1, 1].plot(x, normalized_volume, label='Bitcoin volume', color='blue')\n",
    "axes[1, 1].plot(x, normalized_sentiment, label='Sentiment', color='green')\n",
    "axes[1, 1].set_title('Bitcoin volume vs Sentiment')\n",
    "axes[1, 1].set_xlabel('Weeks')\n",
    "axes[1, 1].set_ylabel('Normalized values')\n",
    "axes[1, 1].legend()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spearman Correlation\n",
    "To be able to detect whether there is correlation between the bitcoin price and the amount of news articles and the bitcoin price and the average news sentiment we will be using spearman correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://en.wikipedia.org/wiki/Spearman's_rank_correlation_coefficient\n",
    "def spearman_correlation(rank1, rank2):\n",
    "    n = len(rank1)\n",
    "\n",
    "    sum_d_squared = sum([(rank1[i] - rank2[i])**2 for i in range(n)])\n",
    "    correlation = 1 - ((6 * sum_d_squared) / (n * (n**2 - 1)))\n",
    "    return correlation\n",
    "\n",
    "\n",
    "# Calculate ranks\n",
    "bp_open_rank = list(weekly_data['open_diff'].dropna().rank())\n",
    "bp_volume_rank = list(weekly_data['volume_diff'].dropna().rank())\n",
    "cn_count_rank = list(weekly_data['count'][1:].rank())\n",
    "cn_sentiment_rank = list(weekly_data['polarity_mean'][1:].rank())\n",
    "\n",
    "pairs = [[bp_open_rank, cn_count_rank], [bp_volume_rank, cn_count_rank],\n",
    "         [bp_open_rank, cn_sentiment_rank], [bp_volume_rank, cn_sentiment_rank]]\n",
    "\n",
    "names = ['open price vs article count', 'volume vs article count',\n",
    "         'open price vs sentiment', 'volume vs sentiment']\n",
    "\n",
    "for i, pair in enumerate(pairs):\n",
    "    correlation = spearman_correlation(pair[0], pair[1])\n",
    "    print(f'{names[i]}')\n",
    "    print(f'Spearman Correlation: {correlation:.3f}\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Granger causality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To determine if one variable causes a change in another variable we are using Granger causality. \\\n",
    "If the p-value of a lag is lower than 0.05 we can determine that this lag influences the variable Thus Granger causes the other varible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_granger_causality(data, lags=5):\n",
    "    granger = grangercausalitytests(data.dropna(), lags)\n",
    "    print(granger)\n",
    "\n",
    "\n",
    "# We use the diffs because the data needs to be stationary.\n",
    "data_pairs = [weekly_data[['count', 'open_diff']],\n",
    "              weekly_data[['count', 'volume_diff']],\n",
    "              weekly_data[['polarity_mean', 'open_diff']],\n",
    "              weekly_data[['polarity_mean', 'volume_diff']]]\n",
    "\n",
    "for data in data_pairs:\n",
    "    test_granger_causality(data, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getVar(data1, data2, n, weekly_data):\n",
    "    data = weekly_data[[data1, data2]].dropna()\n",
    "\n",
    "    model = VAR(data)\n",
    "    lags = {data1: [], data2: []}\n",
    "\n",
    "    for i in range(n):\n",
    "        coef1 = []\n",
    "        coef2 = []\n",
    "        model_fit = model.fit(maxlags=i)\n",
    "\n",
    "        coefs = model_fit.coefs\n",
    "        pvalues = model_fit.pvalues_endog_lagged\n",
    "\n",
    "        for j in range(i):\n",
    "            if pvalues[j*2][0] < 0.05:\n",
    "                coef1.append((j+1, coefs[j][0][0]))\n",
    "            if pvalues[j*2+1][0] < 0.05:\n",
    "                coef2.append((j+1, coefs[j][0][1]))\n",
    "        lags[data1].append(coef1)\n",
    "        lags[data2].append(coef2)\n",
    "    return lags\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getBestLag(data1, data2, n, weekly_data):\n",
    "    min = (0, np.inf)\n",
    "    lags = getVar(data1, data2, n, weekly_data)\n",
    "    for i in range(n):\n",
    "        x_columns_str = ''\n",
    "\n",
    "        for key in lags.keys():\n",
    "            # Skip the empty entries.\n",
    "            if not lags[key]:\n",
    "                continue\n",
    "\n",
    "            print(key, lags[key][i])\n",
    "            for lag, coeff in lags[key][i]:\n",
    "                weekly_data[f'{key}_t-{lag}'] = coeff * weekly_data[f'{key}'].shift(lag)\n",
    "                x_columns_str += f'{key}_t-{lag} '\n",
    "\n",
    "        x_columns_list = x_columns_str.split(' ')[:-1]\n",
    "        # print(x_columns_list)\n",
    "\n",
    "        x = weekly_data[x_columns_list].dropna()\n",
    "        y = weekly_data['open_diff'][len(x_columns_list):]\n",
    "\n",
    "        x = sm.add_constant(x)\n",
    "        print(len(x), len(y), i)\n",
    "        model = sm.OLS(y, x).fit()\n",
    "        min = min if min[1] <= model.aic else (i, model.aic)\n",
    "\n",
    "    return min[0]\n",
    "\n",
    "print(getBestLag('count', 'open_diff', 20, weekly_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combinations = {\n",
    "    ('count', 'open_diff'): [0, 0],\n",
    "    ('count', 'volume_diff'): [0, 0],\n",
    "    ('polarity_mean', 'open_diff'): [0, 0],\n",
    "    ('polarity_mean', 'volume_diff'): [0, 0]\n",
    "}\n",
    "\n",
    "n = 20\n",
    "\n",
    "\n",
    "for data1, data2 in combinations.keys():\n",
    "    combinations[(data1, data2)][0] = getBestLag(data1, data2, n, weekly_data)\n",
    "    combinations[(data1, data2)][1] = getBestLag(data2, data1, n, weekly_data)\n",
    "\n",
    "print(combinations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
